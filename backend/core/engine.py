"""
Inventory processing engine.

This module handles inventory-specific business logic like site metrics collection.
File parsing has been consolidated into parsers.py.
"""
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import zipfile
import xml.etree.ElementTree as ET

# Import parsing utilities from consolidated module
from .parsers import (
    # Utility functions
    sha256_path,
    date_from_filename_or_mtime,
    normalize_text as norm_item,
    to_float,
    # Structured parsing
    parse_excel as parse_excel_file,
    parse_csv as parse_csv_file,
    parse_pdf as parse_pdf_file,
    parse_file,
    # Excel internals (needed for collect_site_metrics)
    NS_MAIN,
    _read_shared_strings as read_shared_strings,
    _read_workbook_sheets as read_workbook_sheets,
    _get_cell_value as get_cell_value,
    _parse_data_sheet as parse_data_sheet,
    # Site extraction
    extract_site_from_excel,
    _extract_site_from_sheet as extract_site_from_sheet,
)

# Re-export for backwards compatibility
__all__ = [
    # Utilities
    "sha256_path",
    "date_from_filename_or_mtime",
    "norm_item",
    "to_float",
    # Parsing
    "parse_excel_file",
    "parse_csv_file",
    "parse_pdf_file",
    "parse_file",
    # Site extraction
    "extract_site_from_sheet",
    # Business logic
    "collect_site_metrics",
]


def collect_site_metrics(site_dir: Path) -> Dict[str, Any]:
    """
    Collect metrics for all inventory files in a site directory.

    Analyzes Excel files to track:
    - Header presence across files
    - Value and quantity drifts between files
    - Latest totals and changes

    Args:
        site_dir: Path to directory containing site inventory files

    Returns:
        Dict with file_summaries, qty_drifts, total_drifts, latest_total, etc.
    """
    files = sorted(site_dir.rglob("*.xlsx"))
    header_keywords = {
        "last inventory qty": ["last inventory qty", "last inv", "last count"],
        "inv count": ["inv count", "inventory count", "count"],
        "price type": ["price type", "pricing basis", "valuation basis"],
        "grouped by": ["grouped by"],
        "sales amount": ["sales amount", "sales", "revenue"],
        "generated date": ["generated date", "run date"],
        "generated by": ["generated by", "run by", "user"],
        "close inventory confirmation": ["close inventory confirmation", "close inventory"],
        "initial inventory": ["initial inventory", "beginning inventory"],
        "ending inventory": ["ending inventory", "end inventory"],
    }

    header_presence = defaultdict(set)
    file_summaries = []
    item_series = defaultdict(dict)

    for p in files:
        try:
            with zipfile.ZipFile(p, "r") as zf:
                shared = read_shared_strings(zf)
                sheets = read_workbook_sheets(zf)

                # Check for headers in first few rows of any sheet
                for name, target in sheets:
                    try:
                        data = zf.read(target)
                    except Exception:
                        continue
                    root_xml = ET.fromstring(data)
                    sheet_data = root_xml.find(f"{{{NS_MAIN}}}sheetData")
                    if sheet_data is None:
                        continue
                    rows = sheet_data.findall(f"{{{NS_MAIN}}}row")[:15]
                    for row in rows:
                        for c in row.findall(f"{{{NS_MAIN}}}c"):
                            val = get_cell_value(c, shared)
                            if not isinstance(val, str) or not val:
                                continue
                            low = val.lower().strip()
                            for key, terms in header_keywords.items():
                                if any(t in low for t in terms):
                                    header_presence[key].add(str(p.relative_to(site_dir)))

                data_sheet = next(
                    (s for s in sheets if s[0].startswith("Data for ")), None
                )
                if not data_sheet:
                    continue
                parsed = parse_data_sheet(zf, data_sheet[1], shared)
                if not parsed:
                    continue
                headers, data_rows = parsed
        except Exception:
            continue

        header_lookup = {v.strip(): k for k, v in headers.items()}
        idx_item = header_lookup.get("Item Description")
        idx_qty = header_lookup.get("Quantity")
        idx_total = header_lookup.get("Total Price")

        totals = []
        for row in data_rows:
            item = row.get(idx_item, "") if idx_item else ""
            item_key = norm_item(item)
            qty = to_float(row.get(idx_qty)) if idx_qty else None
            total = to_float(row.get(idx_total)) if idx_total else None

            if total is not None:
                totals.append(total)

            if item_key:
                entry = item_series[item_key].setdefault(
                    str(p.relative_to(site_dir)),
                    {"item": item, "qty_sum": 0.0, "total_sum": 0.0},
                )
                if qty is not None:
                    entry["qty_sum"] += qty
                if total is not None:
                    entry["total_sum"] += total

        file_summaries.append({
            "path": str(p.relative_to(site_dir)),
            "total_sum": sum(totals) if totals else 0.0,
            "mtime": p.stat().st_mtime,
        })

    # Calculate Drifts
    ordered_paths = [
        f["path"] for f in sorted(file_summaries, key=lambda x: x["mtime"])
    ]
    qty_drifts = []
    total_drifts = []

    for item_key, per_file in item_series.items():
        for i in range(1, len(ordered_paths)):
            prev_path = ordered_paths[i - 1]
            curr_path = ordered_paths[i]
            if prev_path in per_file and curr_path in per_file:
                prev = per_file[prev_path]
                curr = per_file[curr_path]

                # Qty Drift
                if prev["qty_sum"] and curr["qty_sum"]:
                    ratio = curr["qty_sum"] / prev["qty_sum"]
                    if (ratio > 2 or ratio < 0.5) and abs(
                        curr["qty_sum"] - prev["qty_sum"]
                    ) > 10:
                        qty_drifts.append({
                            "item": curr["item"],
                            "prev": prev_path,
                            "curr": curr_path,
                            "prev_qty": prev["qty_sum"],
                            "curr_qty": curr["qty_sum"],
                            "ratio": ratio,
                        })
                # Value Drift
                if prev["total_sum"] and curr["total_sum"]:
                    ratio = curr["total_sum"] / prev["total_sum"]
                    if (ratio > 2 or ratio < 0.5) and abs(
                        curr["total_sum"] - prev["total_sum"]
                    ) > 500:
                        total_drifts.append({
                            "item": curr["item"],
                            "prev": prev_path,
                            "curr": curr_path,
                            "prev_total": prev["total_sum"],
                            "curr_total": curr["total_sum"],
                            "ratio": ratio,
                        })

    qty_drifts.sort(key=lambda x: abs(x["curr_qty"] - x["prev_qty"]), reverse=True)
    total_drifts.sort(
        key=lambda x: abs(x["curr_total"] - x["prev_total"]), reverse=True
    )

    # Latest Stats
    if file_summaries:
        latest = max(file_summaries, key=lambda x: x["mtime"])
        latest_total = latest["total_sum"]
        latest_date = datetime.fromtimestamp(latest["mtime"]).strftime("%Y-%m-%d")

        prev_files = sorted(
            [f for f in file_summaries if f["path"] != latest["path"]],
            key=lambda x: x["mtime"],
        )
        prev_total = prev_files[-1]["total_sum"] if prev_files else 0.0
        delta_pct = (
            ((latest_total - prev_total) / prev_total * 100) if prev_total else 0.0
        )
    else:
        latest_total = 0.0
        delta_pct = 0.0
        latest_date = datetime.now().strftime("%Y-%m-%d")

    missing_fields = [
        k for k in header_keywords.keys() if not header_presence.get(k)
    ]

    return {
        "file_summaries": file_summaries,
        "qty_drifts": qty_drifts,
        "total_drifts": total_drifts,
        "latest_total": latest_total,
        "delta_pct": delta_pct,
        "latest_date": latest_date,
        "missing_fields": missing_fields,
    }
